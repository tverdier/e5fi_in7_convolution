{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IN7 - IA pour l'image - Projet\n",
    "## Equipe 11 - Céline NGUYEN, Pierre LEBAS, Thibault VERDIER et Pierre MESTRE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "## 1.1 Objectif\n",
    "L’objectif de ce Projet est de développer un ensemble de classifieurs supervisés Machine Learning et\n",
    "Deep Learning :\n",
    "- Chargement d’une base d’images,\n",
    "- Entrainement d’un modèle de classification supervisée à l’aide d’une base d’apprentissage,\n",
    "- Validation et test du modèle entrainé sur un nouveau jeu d’images,\n",
    "- Evaluation des résultats obtenus et amélioration des performances si besoin.\n",
    "Vous trouvez en ce qui suit des indications et exemples de codes.\n",
    "\n",
    "## 1.2 Datasets\n",
    "Trois bases d’images seront utilisées pour la réalisation de ce projet ;\n",
    "- La base MNIST / DGITS :"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from keras.datasets import mnist\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Deux bases Kaggle_ImageNet :\n",
    "    - La première contient 10000 images réparties sur deux classes « Dogs&Cats » : https://www.kaggle.com/chetankv/dogs-cats-images\n",
    "    - et la deuxième contient 25000 images réparties sur plusieurs classes « Intel Image Classification ; Image Scene Classification of Multiclass ». Pour le chargement de ces bases, la commande os.listdir du package « os » sera utilisée : https://www.ted.com/talks/fei_fei_li_how_we_re_teaching_computers_to_understand_pictures?language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. ML _ Extraction du descripteur ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "orb=cv2.ORB_create()\n",
    "\n",
    "im=cv2.imread( ???)\n",
    "\n",
    "plt.imshow(im)\n",
    "\n",
    "# Function for plotting keypoints\n",
    "def draw_keypoints(vis, keypoints, color = (0, 255, 255)):\n",
    "    for kp in keypoints:\n",
    "        x, y = kp.pt\n",
    "        plt.imshow(cv2.circle(vis, (int(x), int(y)), 2, color))\n",
    "        \n",
    "# Plotting the keypoints\n",
    "kp = orb.detect(im,None)\n",
    "kp, des = orb.compute(im, kp)\n",
    "\n",
    "img = draw_keypoints(im, kp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ML _ Classification supervisé"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "#img = cv2.imread('OpenCV.PNG')\n",
    "img = cv2.imread('lena.jpg')\n",
    "Z = img.reshape((-1,3))\n",
    "\n",
    "# convert to np.float32\n",
    "Z = np.float32(Z)\n",
    "pour une base\n",
    "\n",
    "# define criteria, number of clusters(K) and apply kmeans()\n",
    "criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "K = 10\n",
    "ret,label,center=cv2.kmeans(Z,K,None,criteria,10,cv2.KMEANS_RANDOM_CENTERS)\n",
    "\n",
    "# Now convert back into uint8, and make original image\n",
    "center = np.uint8(center)\n",
    "res = center[label.flatten()]\n",
    "res2 = res.reshape((img.shape))\n",
    "\n",
    "cv2.imshow('Image',img)\n",
    "cv2.imshow('res2',res2)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### LIBRARY\n",
    "import os\n",
    "import tensorflow\n",
    "import keras\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as matplot\n",
    "import matplotlib\n",
    "import seaborn as sb\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATASET ONE #######\n",
    "from keras.datasets import mnist\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RESHAPE 2D\n",
    "\n",
    "nsamples, nx, ny = X_train.shape\n",
    "d1_train_dataset = X_train.reshape((nsamples, nx*ny))\n",
    "\n",
    "nsamples, nx, ny = X_test.shape\n",
    "d1_test_dataset = X_test.reshape((nsamples, nx*ny))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_train_dataset[:500].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1_train_dataset[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to List all the filenames in the directory\n",
    "def img_list(path):\n",
    "    return (os.path.join(path,f) for f in os.listdir(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####UTILISATION MODELE SVM\n",
    "### DATASET 1\n",
    "from sklearn.svm import SVC, LinearSVC # Support Vector for Classification\n",
    "\n",
    "\n",
    "# param_C = 5\n",
    "# param_gamma = 0.05\n",
    "# classifier = SVC(C=param_C,gamma=param_gamma)\n",
    "classifier = sklearn.linear_model.SGDClassifier()#SVC(kernel = 'linear', random_state = 0)\n",
    "# classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(d1_train_dataset, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = classifier.predict(d1_test_dataset) # prédiction à partir de notre modèle entraîné sur le jeu de test\n",
    "predicted_y_train = classifier.predict(d1_train_dataset) # prédiction à partir de notre modèle entraîné sur le jeu de training (pour vérifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training data is\",accuracy_score(y_train, predicted_y_train)) #affichage du score de précision obtenu sur le jeu de training qui est de 99%\n",
    "print(\"Accuracy on unknown data is\",accuracy_score(y_test, predicted_y)) #affichage du score de précision obtenu sur le jeu de test \"inconnu\" qui est de 38%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test, predicted_y)\n",
    "\n",
    "matplot.subplots(figsize=(10, 6))\n",
    "sb.heatmap(cm, annot = True, fmt = 'g')\n",
    "matplot.xlabel(\"Predicted\")\n",
    "matplot.ylabel(\"Actual\")\n",
    "matplot.title(\"Confusion Matrix\")\n",
    "matplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### DATASET TWO #########\n",
    "###########################\n",
    "train_path = \"dataset/dog_cat/training_set\"\n",
    "test_path = \"dataset/dog_cat/test_set\"\n",
    "\n",
    "class_names=os.listdir(train_path)\n",
    "\n",
    "liste_dict = {}\n",
    "for i in range(len(class_names)):\n",
    "    liste_dict[class_names[i]] = int(i)\n",
    "\n",
    "\n",
    "print(class_names) # ==> ['dogs', 'cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dogcat = pd.DataFrame(columns=[\"Image_path\", \"Class\", \"Train/Test\"])\n",
    "\n",
    "for training_name in class_names:\n",
    "    dir_=os.path.join(train_path,training_name)\n",
    "    class_path=img_list(dir_)\n",
    "#     for image_path in class_path:\n",
    "#         dataset_dogcat = dataset_dogcat.append({\"Image_path\":image_path, \"Y_value\": liste_dict[training_name], \"Class\": training_name,  \"Train/Test\": \"train\"}, ignore_index = True)\n",
    "    array = [d for d in class_path]\n",
    "    df = pd.DataFrame({\"Image_path\": [d for d in array], \"Y_value\": liste_dict[training_name], \"Class\": training_name,  \"Train/Test\": \"train\"})    \n",
    "    dataset_dogcat = dataset_dogcat.append(df, ignore_index=True)\n",
    "    \n",
    "    \n",
    "for test_name in class_names:\n",
    "    dir_=os.path.join(test_path,test_name)\n",
    "    class_path=img_list(dir_)\n",
    "#     for image_path in class_path:\n",
    "#         dataset_dogcat = dataset_dogcat.append({\"Image_path\":image_path, \"Y_value\": liste_dict[test_name], \"Class\": test_name,  \"Train/Test\": \"test\"}, ignore_index = True)\n",
    "    array = [d for d in class_path]\n",
    "    df = pd.DataFrame({\"Image_path\": [d for d in array], \"Y_value\": liste_dict[test_name], \"Class\": test_name,  \"Train/Test\": \"test\"})    \n",
    "    dataset_dogcat = dataset_dogcat.append(df, ignore_index=True)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dogcat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data2 = []\n",
    "y_train_data2 = dataset_dogcat[dataset_dogcat[\"Train/Test\"] == \"train\"][\"Y_value\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_dogcat[dataset_dogcat[\"Train/Test\"] == \"train\"][\"Image_path\"])):\n",
    "    im = cv2.imread(dataset_dogcat[dataset_dogcat[\"Train/Test\"] == \"train\"][\"Image_path\"].iloc[i])\n",
    "    X_train_data2.append(cv2.resize(im, image_size))  #pour chaque image du dataset, on redimensionne à la taille image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray(X_train_data2)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images =  np.reshape(images, \n",
    "                     (images.shape[0], images.shape[1]*images.shape[2]*images.shape[3]))\n",
    "#aplati les données des images : on va réduire les dimensions de notre dataset en un tableau 2D\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data2 = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data2 = []\n",
    "y_test_data2 = dataset_dogcat[dataset_dogcat[\"Train/Test\"] == \"test\"][\"Y_value\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_dogcat[dataset_dogcat[\"Train/Test\"] == \"test\"][\"Image_path\"])):\n",
    "    im = cv2.imread(dataset_dogcat[dataset_dogcat[\"Train/Test\"] == \"test\"][\"Image_path\"].iloc[i])\n",
    "    X_test_data2.append(cv2.resize(im, image_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray(X_test_data2)\n",
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images =  np.reshape(images, \n",
    "                     (images.shape[0], images.shape[1]*images.shape[2]*images.shape[3]))\n",
    "#aplati les données des images : on va réduire les dimensions de notre dataset en un tableau 2D\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data2 = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.linear_model.SGDClassifier()#SVC(kernel = 'linear', random_state = 0)\n",
    "# classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train_data2, y_train_data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = classifier.predict(X_test_data2) # prédiction à partir de notre modèle entraîné sur le jeu de test\n",
    "predicted_y_train = classifier.predict(X_train_data2) # prédiction à partir de notre modèle entraîné sur le jeu de training (pour vérifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training data is\",accuracy_score(y_train_data2, predicted_y_train,)) #affichage du score de précision obtenu sur le jeu de training qui est de 99%\n",
    "print(\"Accuracy on unknown data is\",accuracy_score(y_test_data2, predicted_y)) #affichage du score de précision obtenu sur le jeu de test \"inconnu\" qui est de 38%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test_data2, predicted_y)\n",
    "\n",
    "matplot.subplots(figsize=(10, 6))\n",
    "sb.heatmap(cm, annot = True, fmt = 'g')\n",
    "matplot.xlabel(\"Predicted\")\n",
    "matplot.ylabel(\"Actual\")\n",
    "matplot.title(\"Confusion Matrix\")\n",
    "matplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "##### DATASET THREE #######\n",
    "###########################\n",
    "train_path_intel = \"dataset/intel_image_classification/seg_train/seg_train\"\n",
    "test_path_intel = \"dataset/intel_image_classification/seg_test/seg_test\"\n",
    "pred_path_intel = \"dataset/intel_image_classification/seg_pred/seg_pred\"\n",
    "\n",
    "class_names_intel = os.listdir(train_path_intel)\n",
    "\n",
    "liste_dict_intel = {}\n",
    "for i in range(len(class_names_intel)):\n",
    "    liste_dict_intel[class_names_intel[i]] = int(i)\n",
    "\n",
    "print(class_names_intel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_intel = pd.DataFrame(columns=[\"Image_path\", \"Class\", \"Train/Test/Pred\"])\n",
    "\n",
    "for training_name in class_names_intel:\n",
    "    dir_=os.path.join(train_path_intel, training_name)\n",
    "    class_path=img_list(dir_)\n",
    "    array = [d for d in class_path]\n",
    "    df = pd.DataFrame({\"Image_path\": [d for d in array], \"Y_value\": liste_dict_intel[training_name], \"Class\": training_name,  \"Train/Test/Pred\": \"train\"})    \n",
    "    dataset_intel = dataset_intel.append(df, ignore_index=True)\n",
    "\n",
    "\n",
    "for test_name in class_names_intel:\n",
    "    dir_=os.path.join(test_path_intel, test_name)\n",
    "    class_path=img_list(dir_)\n",
    "    array = [d for d in class_path]\n",
    "    df = pd.DataFrame({\"Image_path\": [d for d in array], \"Y_value\": liste_dict_intel[training_name], \"Class\": test_name,  \"Train/Test/Pred\": \"test\"})    \n",
    "    dataset_intel = dataset_intel.append(df, ignore_index=True)\n",
    "\n",
    "\n",
    "## à prédire\n",
    "class_path=img_list(pred_path_intel)\n",
    "array = [d for d in class_path]\n",
    "df = pd.DataFrame({\"Image_path\": [d for d in array], \"Y_value\": \"\", \"Class\": \"\",  \"Train/Test/Pred\": \"pred\"})    \n",
    "dataset_intel = dataset_intel.append(df, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_data3 = []\n",
    "y_train_data3 = dataset_intel[dataset_intel[\"Train/Test/Pred\"] == \"train\"][\"Y_value\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = (150, 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_intel[dataset_intel[\"Train/Test/Pred\"] == \"test\"][\"Image_path\"])):\n",
    "    im = cv2.imread(dataset_intel[dataset_intel[\"Train/Test/Pred\"] == \"test\"][\"Image_path\"].iloc[i])\n",
    "    X_test_data3.append(cv2.resize(im, image_size))  #pour chaque image du dataset, on redimensionne à la taille image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray(X_test_data3)\n",
    "print(images.shape)\n",
    "\n",
    "images =  np.reshape(images, \n",
    "                     (images.shape[0], images.shape[1]*images.shape[2]*images.shape[3]))\n",
    "#aplati les données des images : on va réduire les dimensions de notre dataset en un tableau 2D\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data3 = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data3 = []\n",
    "y_test_data3 = dataset_intel[dataset_intel[\"Train/Test/Pred\"] == \"test\"][\"Y_value\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_intel[dataset_intel[\"Train/Test/Pred\"] == \"test\"][\"Image_path\"])):\n",
    "    im = cv2.imread(dataset_intel[dataset_intel[\"Train/Test/Pred\"] == \"test\"][\"Image_path\"].iloc[i])\n",
    "    X_test_data3.append(cv2.resize(im, image_size))  #pour chaque image du dataset, on redimensionne à la taille image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray(X_test_data3)\n",
    "print(images.shape)\n",
    "\n",
    "images =  np.reshape(images, \n",
    "                     (images.shape[0], images.shape[1]*images.shape[2]*images.shape[3]))\n",
    "#aplati les données des images : on va réduire les dimensions de notre dataset en un tableau 2D\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data3 = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_data3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset_intel[dataset_intel[\"Train/Test/Pred\"] == \"pred\"][\"Image_path\"])):\n",
    "    im = cv2.imread(dataset_intel[dataset_intel[\"Train/Test/Pred\"] == \"pred\"][\"Image_path\"].iloc[i])\n",
    "    X_pred_data3.append(cv2.resize(im, image_size))  #pour chaque image du dataset, on redimensionne à la taille image_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.asarray(X_pred_data3)\n",
    "print(images.shape)\n",
    "\n",
    "images =  np.reshape(images, \n",
    "                     (images.shape[0], images.shape[1]*images.shape[2]*images.shape[3]))\n",
    "#aplati les données des images : on va réduire les dimensions de notre dataset en un tableau 2D\n",
    "print(images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_data3 = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = sklearn.linear_model.SGDClassifier()#SVC(kernel = 'linear', random_state = 0)\n",
    "# classifier = SVC(kernel = 'rbf', random_state = 0)\n",
    "classifier.fit(X_train_data3, y_train_data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = classifier.predict(X_test_data3) # prédiction à partir de notre modèle entraîné sur le jeu de test\n",
    "predicted_y_train = classifier.predict(X_train_data3) # prédiction à partir de notre modèle entraîné sur le jeu de training (pour vérifier)\n",
    "predicted_y_pred = classifier.predict(X_pred_data3) # prédiction à partir de notre modèle entraîné sur le jeu de training (pour vérifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on training data is\",accuracy_score(y_train_data3, predicted_y_train,)) #affichage du score de précision obtenu sur le jeu de training\n",
    "print(\"Accuracy on test data is\",accuracy_score(y_test_data3, predicted_y)) #affichage du score de précision obtenu sur le jeu de test\n",
    "# print(\"Accuracy on unknown data is\",accuracy_score(y_test_data3, predicted_y)) #affichage du score de précision obtenu sur le jeu de test \"inconnu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de confusion\n",
    "cm = confusion_matrix(y_test_data3, predicted_y)\n",
    "\n",
    "matplot.subplots(figsize=(10, 6))\n",
    "sb.heatmap(cm, annot = True, fmt = 'g')\n",
    "matplot.xlabel(\"Predicted\")\n",
    "matplot.ylabel(\"Actual\")\n",
    "matplot.title(\"Confusion Matrix\")\n",
    "matplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste_dict_intel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Sac de mots visuels modèle (BOW) « Bag of Word »"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating histogram of training image\n",
    "from scipy.cluster.vq import ,vq\n",
    "k=200\n",
    "im_features=np.zeros((len(image_dataset),k),\"float32\")\n",
    "for i in range(len(image_dataset)):\n",
    "words,distance=vq(image_descriptor[i],voc) # Voc représente le dictionnaire préalablement #\n",
    "construit\n",
    "for w in words:\n",
    "im_features[i][w]+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. DL_CNN «Convolutional_Neural_Networks»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the CNN\n",
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1 - Convolution\n",
    "classifier.add(Conv2D(32, (3, 3), input_shape = (64, 64, 3), activation = 'relu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding a second convolutional layer\n",
    "classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "classifier.add(MaxPooling2D(pool_size = (2, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3 - Flattening\n",
    "classifier.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4 - Full connection\n",
    "classifier.add(Dense(units = 128, activation = 'relu'))\n",
    "classifier.add(Dense(units = 1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ....\n",
    "classifier.fit_generator(training_set, steps_per_epoch = 8000, epochs = 25, validation_data = test_set, validation_steps = 2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy=accuracy_score(true_classes,predict_classes)\n",
    "print(accuracy)\n",
    "\n",
    "# Matrice de confusion\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
